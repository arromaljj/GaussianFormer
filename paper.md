Thanks for sharing your background and learning goals! I'll break down this research paper thoroughly, explaining all the core concepts, computations, and ideas it builds upon, while also detailing how everything fits together architecturally. I'll provide an in-depth analysis of both the inference and training pipelines, ensuring clarity on the entire framework. I'll get back to you with a structured explanation soon.

# Core Concepts and Background

**3D Semantic Occupancy Prediction:** In autonomous driving, **3D semantic occupancy prediction** means predicting, for each location in a 3D volume around the vehicle, whether that location is occupied and what semantic class it belongs to (e.g. car, pedestrian, road) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). Essentially, the model produces a dense 3D grid (voxels) with class labels, providing fine-grained geometry and semantics of the scene. This is important for vision-based self-driving because cameras alone do not directly give 3D structure like LiDAR does. By predicting occupancy in 3D, a vision system can infer the shapes of obstacles and free space, improving safety and robustness ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=3D%20semantic%20occupancy%20prediction%20aims,attention%20mechanism%20and%20iteratively%20refine)). For example, the car can anticipate the full 3D shape of a vehicle or a curb from images, enabling better path planning. Occupancy grids are a comprehensive scene representation – richer than 2D detection or segmentation – since they account for *all* space (including unseen or occluded regions) and assign semantic labels to it.

**Traditional Approaches (Voxel Grids and BEV):** Early approaches represent the scene with **dense 3D voxel grids**, where each voxel holds features or a predicted label. While straightforward, voxel grids are extremely memory- and compute-intensive because most voxels (especially in outdoor scenes) are empty air ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=The%20autonomous%20driving%20scenes%20contain,based%20methods%20can%20hardly)). Treating every voxel equally wastes resources, since a huge number of voxels contain no relevant objects. It also struggles with representing objects of varying size: a pedestrian and a bus are represented with the same voxel size resolution. On the other hand, **planar projections** like Bird's-Eye View (BEV) compress the 3D scene into a 2D map (top-down) and perform prediction on this plane ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=because%20of%20unreasonable%20resource%20allocation,to%20representation%20and%20computation%20redundancy)). BEV and related multi-plane methods (e.g. TPV, tri-perspective view) use fewer cells than a full 3D grid, improving efficiency ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=because%20of%20unreasonable%20resource%20allocation,to%20representation%20and%20computation%20redundancy)). However, they sacrifice detail – collapsing the height dimension can cause loss of fine vertical structure (for instance, a hanging traffic light or an overpass might be hard to distinguish in BEV). Figure 2 visualizes these differences. **Voxel grids** (left of Figure 2) partition the space uniformly but lead to many unnecessary cells, whereas **BEV** and **TPV** (middle) encode the scene on one or few planes, saving memory but blurring some 3D details ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=BEV%C2%A0,to%20representation%20and%20computation%20redundancy)). In short, grid-based representations suffer from redundancy or information loss, and cannot easily adapt to the varying scales and sparsity of objects in driving scenes ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=structures%20from%202D%20features,to%20representation%20and%20computation%20redundancy)).

 ([GaussianFormer](https://wzzheng.net/GaussianFormer/)) *Figure 2: Comparison of scene representations. Left: a dense **voxel** grid treats every 3D location equally, which is wasteful in empty regions ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=representation%20with%20exiting%20grid,2%2C%2011)). Middle: planar representations like **BEV** (bird's-eye view) or **TPV** (multi-plane) reduce dimensionality, using 2D grids to describe 3D, but at some cost of detail ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=representation%20with%20exiting%20grid,2%2C%2011)). Right: the proposed **3D Gaussian** representation models the scene as a set of ellipsoid Gaussian volumes fitted to objects/regions, capturing fine geometry with far fewer units ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=of%20the%203D%20space.%20BEV%C2%A0,8%20%2C%20%203)). Each colored ellipsoid represents a Gaussian that can flexibly cover a region of interest in the scene.* ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=of%20the%203D%20space.%20BEV%C2%A0,8%20%2C%20%203)) ([GaussianFormer](https://wzzheng.net/GaussianFormer/#:~:text=The%20voxel%20representation%20assigns%20each,approximating%20ability%20of%20mixing%20Gaussians))

**Gaussian Mixture Models for 3D Representation:** Instead of fixed grids, *GaussianFormer* introduces an **object-centric representation** where the scene is described by a set of 3D Gaussian functions ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=To%20address%20this%2C%20we%20propose,and%20%2C%20%2C)). This draws on the idea of **Gaussian Mixture Models (GMMs)** – a technique where complex shapes or distributions are represented as a sum of multiple Gaussians. Gaussians are bell-shaped functions that can be stretched (scaled) and rotated to fit different parts of a shape. A fundamental property of Gaussian mixtures is their **universal approximation ability**, meaning a sufficient number of Gaussians can approximate arbitrary shapes or functions ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=of%20the%203D%20space.%20BEV%C2%A0,8%20%2C%20%203)). In this context, each Gaussian can occupy a "blob" of space covering part of an object or region. Because Gaussians have continuous support and can vary in size, a sparse set of them can capture both large structures (e.g. a floor or wall using a broad Gaussian) and fine details (e.g. a thin pole using a narrow Gaussian) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=object,8%20%2C%20%203)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=scene,right%20corner%20of%20the%203D)). This adaptive resolution is a key advantage: the representation naturally allocates more Gaussians to complex or crowded areas and fewer to empty or simple areas, focusing computational resources where needed ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=scene,right%20corner%20of%20the%203D)). Each **3D Gaussian** in the model is characterized by a mean position (center in 3D), a covariance matrix (defining its size/extent in each direction), and a semantic label distribution ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=fixed%20grids%2C%20as%20shown%20in,distribution%20evaluated%20at%20point%20is)). Intuitively, you can imagine covering the scene with a bunch of ellipsoidal "pillow" shapes – some big, some small, oriented differently – such that when added together, they mimic the actual scene geometry (see the rightmost part of Figure 2). The authors represent each Gaussian by a feature vector that includes its center coordinates, scale (size radii), orientation (rotation), and semantic class scores ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=fixed%20grids%2C%20as%20shown%20in,distribution%20evaluated%20at%20point%20is)).

**Gaussian Splatting:** Once the scene is represented as a collection of Gaussians, the model still needs to produce a *voxel grid output* (for evaluation and downstream usage). The process of **Gaussian-to-voxel splatting** refers to converting the continuous Gaussian representation into a discrete occupancy map. **Splatting** is a term borrowed from graphics, meaning to *spread or project* a primitive onto discrete pixels/voxels. Here, each 3D Gaussian contributes to nearby voxels according to its density. Mathematically, a Gaussian centered at **μ** with covariance **Σ** assigns a weight to a point **p** proportional to $\exp\!\big(-\tfrac{1}{2}(p-\mu)^T \Sigma^{-1}(p-\mu)\big)$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Although%20,voxel%20position%20to%20improve%20efficiency)). This weight decays exponentially with the squared Mahalanobis distance (distance in the Gaussian's frame), so a voxel far from the Gaussian's center gets almost zero contribution ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Gaussians%20for%20every%20voxel%20position,voxel%20position%20to%20improve%20efficiency)). To "splat" the Gaussian into the voxel grid, we find all voxels within some radius of the Gaussian's mean (where the weight isn't negligible) and add the Gaussian's semantic probabilities to those voxels ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=respect%20to%20the%20square%20of,voxel%20position%20to%20improve%20efficiency)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). The occupancy value for a voxel is then the sum of contributions from all Gaussians nearby ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20%2C%20and%20represent%20the,Gaussians%20on%20the%20location)). In practice, an efficient indexing and aggregation strategy is used: each Gaussian "registers" itself to the list of voxels it influences, and by sorting these, the model quickly looks up which Gaussians affect each voxel ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=As%20illustrated%20by%20Fig,each%20voxel%20should%20attend%20to)). This **Gaussian splatting** technique is much more efficient than naively checking every Gaussian for every voxel (which would be infeasible) – it leverages the locality of Gaussian influence ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Although%20,voxel%20position%20to%20improve%20efficiency)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)).

**Attention Mechanisms in Deep Learning:** *GaussianFormer* uses transformer-like **attention modules** to connect image features with the 3D Gaussians. An **attention mechanism** allows a model to focus on relevant parts of its input by computing weighted averages of information. In an attention module, a set of **queries** attends to a set of **key-value pairs**, producing an output for each query that is a weighted sum of the values, where weights are determined by query-key similarity. This mechanism has proven very effective in tasks from machine translation to vision, because it can flexibly **aggregate information** from different locations ([11.1. Queries, Keys, and Values — Dive into Deep Learning 1.0.3 documentation](https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html#:~:text=The%20attention%20mechanism%20allows%20us,here%3A%20for%20instance%2C%20in%20a)). In our context, each 3D Gaussian acts like a query that looks at 2D image feature maps (which serve as keys/values) to find which image regions correspond to that Gaussian. This is done via a **cross-attention** module: the Gaussian (query) projects to certain image positions and "attends" to the features at those locations ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Image%20Cross,sum%20of%20retrieved%20image%20features)). Attention enables the model to pull only the pertinent visual features for each object-centric unit, rather than relying on a fixed receptive field. Additionally, *GaussianFormer* uses a form of **self-attention** (within the set of Gaussians) to allow Gaussians to interact with each other's information, so that, for example, neighboring Gaussians can exchange context. Overall, attention provides a learnable way to route information: the network can learn *which* image pixels correspond to a given 3D location (through cross-attention) and how different Gaussians relate in space (through self-encoding interactions). This is more flexible than manual alignment; it's akin to how the DETR object detection model uses learned queries to attend to image features for each potential object ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Refinement%20Module,intermediate%20ones%2C%20we%20treat%20the)). The result is an **object-centric perception** pipeline where each Gaussian picks up visual evidence for a region and refines itself, rather than the model dense-predicting every voxel from images directly.

<br>

# Architectural Breakdown

**Object-Centric 3D Gaussian Representation:** *GaussianFormer* describes the 3D scene as a collection of **3D semantic Gaussians**, each acting as an individual unit or "object" ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=To%20address%20this%2C%20we%20propose,and%20%2C%20%2C)). Unlike a voxel grid where each cell is a fixed tiny cube, a Gaussian can be thought of as an adjustable ellipsoid that covers a region of space. Formally, a single Gaussian $G_i$ is defined by: a mean position $\mu_i = (x_i,y_i,z_i)$, a scale vector $s_i = (s_{ix}, s_{iy}, s_{iz})$ (which relates to the covariance or radius in each axis), an orientation (often parameterized by a quaternion $q_i$ or rotation matrix $R_i$), and a set of semantic logits $l_i$ for the classes ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=fixed%20grids%2C%20as%20shown%20in,distribution%20evaluated%20at%20point%20is)). These parameters determine a 3D Gaussian spatial function: 

$$G_i(p) = \exp\!\big(-\tfrac{1}{2}(p-\mu_i)^T \Sigma_i^{-1}(p-\mu_i)\big)$$

where 

$$\Sigma_i = R_i\,\mathrm{Diag}(s_i)\,R_i^T$$

is the covariance matrix constructed from the scale and rotation ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). Importantly, $l_i$ associates class scores with this Gaussian – you can think of it as the Gaussian "preferring" a certain semantic category (or a distribution over categories). To get a semantic occupancy prediction at any point **p** in space, the contributions of all Gaussians are **summed up**: 

$$F(p) = \sum_i \text{softmax}(l_i)\, G_i(p)$$

yielding a fused probability over classes ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20%2C%20and%20represent%20the,Gaussians%20on%20the%20location)). In simpler terms, each Gaussian is like a colored translucent blob (color representing semantic class) in space; the opacity of that blob at a given location p depends on the Gaussian's distance to p, and overlapping blobs add up. This representation is beneficial because of its **adaptivity**: the model can use a *few large Gaussians* to cover broad simple areas (like flat road or ground) and *many small Gaussians* to cover detailed or crowded areas (like a cluster of pedestrians) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=scene,right%20corner%20of%20the%203D)). Therefore, memory and computation are focused on the complex parts of the scene, addressing the uneven resource allocation problem of dense grids ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Compared%20with%20voxel%20representation%2C%20the,those%20in%20other%20representations%20which)). Moreover, each Gaussian inherently carries a semantic label, so converting the representation to an occupancy map is straightforward (just evaluating Gaussians); there's no need for a heavy decoder to interpret features as in other methods ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=fewer%203D%20Gaussians%20to%20model,semantics%20from%20high%20dimensional%20features)).

**Overall Model Structure:** The **GaussianFormer model** takes multi-view images as input and outputs a set of refined 3D Gaussians, which are then splatted into a voxel grid. It does so through an iterative, transformer-like architecture composed of specialized modules. *Figure 3* below illustrates the pipeline. First, an **image encoder** (a CNN with a Feature Pyramid Network) processes the input images to extract multi-scale 2D feature maps ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Image%3A%20Refer%20to%20caption%20Figure,via%20local%20aggregation%20of%20Gaussians)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=We%20set%20the%20resolutions%20of,with%20a%20weight%20decay%20of)). Then, a fixed number of **initial Gaussian queries** are introduced – essentially, the model starts with a set of learnable "proposals" for Gaussians with random initial properties (positions, sizes, etc.) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=image%20inputs%20using%20an%20image,via%20local%20aggregation%20of%20Gaussians)). These go through $B$ rounds (blocks) of refinement. Each **refinement block** consists of three stages: (1) a **Self-encoding** stage where Gaussians interact with each other, (2) an **Image Cross-Attention** stage where Gaussians retrieve relevant information from the image features, and (3) a **Refinement** stage where each Gaussian's parameters (mean, covariance, semantics) are updated based on the gathered information ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=properties%20of%203D%20Gaussians%20and,the%20properties%20of%203D%20Gaussians)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=blocks%20of%20GaussianFormer,the%20properties%20of%203D%20Gaussians)). After $B$ such blocks, the Gaussians are much more "aligned" to the actual scene. Finally, a **Gaussian-to-voxel splatting** module converts this set of Gaussians into the dense 3D occupancy grid by local aggregation (as described earlier) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=image%20inputs%20using%20an%20image,via%20local%20aggregation%20of%20Gaussians)). The model is trained end-to-end, with losses driving the Gaussians to represent the ground truth occupancy at each refinement step.

 ([GaussianFormer](https://wzzheng.net/GaussianFormer/)) *Figure 3: **GaussianFormer Architecture.** The model transforms multi-view **image input** into an **object-centric 3D Gaussian** representation and then into a dense occupancy map. First, an **Image Encoder** (e.g. ResNet+FPN) produces multi-scale (M.S.) feature maps from the images ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Image%3A%20Refer%20to%20caption%20Figure,via%20local%20aggregation%20of%20Gaussians)). We initialize a set of learnable **queries** which correspond to initial 3D Gaussians with random properties (illustrated as colored boxes and a cloud of points) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=image%20inputs%20using%20an%20image,via%20local%20aggregation%20of%20Gaussians)). Each of $B$ transformer blocks applies: a **Self-encoding** module (left blue box) where Gaussians are voxelized into a sparse grid and processed with sparse convolutions ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Self,sparse%20convolution%20could%20effectively%20take)); an **Image Cross-Attention** module (middle green box) where each Gaussian uses reference points (offset from its mean) to sample and attend to 2D image features ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Image%20Cross,sum%20of%20retrieved%20image%20features)); and a **Refinement** module (right yellow box) where the Gaussian's query features predict adjustments to its pose and semantics (using a DETR-inspired update with residuals) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Refinement%20Module,intermediate%20ones%2C%20we%20treat%20the)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). After iterative refinement through these blocks, we obtain an updated set of Gaussians (depicted as ellipsoids in different colors). Finally, the **Gaussian-to-Voxel Splatting** module aggregates contributions of these Gaussians into a 3D occupancy grid (purple voxels), which is the output semantic occupancy map ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=image%20inputs%20using%20an%20image,via%20local%20aggregation%20of%20Gaussians)).*

**Self-encoding Module (Gaussian-Gaussian Interaction):** Rather than using costly global self-attention among potentially tens of thousands of Gaussians, GaussianFormer leverages **3D sparse convolution** as a way for Gaussians to interact efficiently ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Self,sparse%20convolution%20could%20effectively%20take)). The idea is to treat the current set of Gaussian centers as points in space, and rasterize them into a sparse 3D grid. Concretely, each Gaussian (at mean $\mu$) is mapped to the nearest voxel cell, and that cell is given a feature vector (for example, the Gaussian's query feature) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=considerations%2C%20which%20is%20not%20well,of%20the%20sparsity%20of%20Gaussians)). This creates a sparse voxel cloud: only voxels where a Gaussian exists are "occupied" with features, all other voxels are empty. A 3D **sparse convolution** (using a library like Minkowski Engine) is then applied. Sparse conv will slide filters over this sparse voxel set, aggregating information from neighboring Gaussian-points – effectively a learned message passing between nearby Gaussians. This approach has the same goal as self-attention (to mix information between elements), but at a fixed local neighborhood scale and linear complexity ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Self,sparse%20convolution%20could%20effectively%20take)). Notably, the computation cost scales with the number of Gaussians (sparse points) and their local density, not the full volume. Since the number of Gaussians is much smaller than total voxels in a dense grid, this is far more efficient than operating on a full occupancy grid ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=among%203D%20Gaussians%2C%20sharing%20the,of%20the%20sparsity%20of%20Gaussians)). In fact, the authors point out that this achieves **linear complexity** similar to deformable attention used in prior occupancy networks ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Self,sparse%20convolution%20could%20effectively%20take)). The output of the self-encoding module is that each Gaussian's feature (query) is now enriched with context from other Gaussians around it – for example, a Gaussian representing part of a car may receive info from nearby Gaussians representing the same car or the ground below it.

**Image Cross-Attention Module:** After self-encoding, each Gaussian query next gathers visual evidence from the images. The model uses an **Image Cross-Attention (ICA)** mechanism tailored to multi-view camera input ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Image%20Cross,sum%20of%20retrieved%20image%20features)). For each Gaussian (query), a set of **3D reference points** is generated around its mean. These points are essentially sample positions in 3D that the Gaussian will project into the images to see what's there. How are they chosen? The paper says they **permute the mean with offsets determined by the Gaussian's covariance** ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=to%20extract%20visual%20information%20from,sum%20of%20retrieved%20image%20features)). In other words, they take the Gaussian's center and add a few offsets (deltas) in different directions (likely along principal axes of its ellipsoid, e.g. forward, backward, up, down relative to its orientation) so that the points roughly cover the spatial extent of that Gaussian. These reference points are then projected into each camera view using the known camera **intrinsics and extrinsics** (i.e. 3D→2D projection) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=elaborate%2C%20for%20a%203D%20Gaussian,sum%20of%20retrieved%20image%20features)). Once we know where these 3D points fall on each image, we can sample the **image feature maps** at those locations. The cross-attention module uses a deformable attention approach: it looks up multi-scale features at the projected locations and computes a weighted sum to update the Gaussian's query feature ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=covariance%20of%20the%20Gaussian%20to,sum%20of%20retrieved%20image%20features)). Equation (4) in the paper formalizes this step, essentially saying that the Gaussian's new feature is obtained by attending to the set of image features at the projected reference points across all views ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). Intuitively, each 3D Gaussian "looks" into the camera images to see what color/texture/edge features exist at its location. If a Gaussian is, say, near a car's surface, it will project onto the car's image regions and thus pull in features indicative of "car" (like edges, color). Because multiple reference points are used, the Gaussian can gather a rough shape outline from the images. This cross-attention is what injects **visual semantics** into the otherwise geometry-based Gaussian representation.

**Refinement Module:** After each Gaussian has aggregated context from other Gaussians (self-encoding) and information from images (cross-attention), the **refinement module** updates the Gaussian's parameters ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Refinement%20Module,intermediate%20ones%2C%20we%20treat%20the)). The Gaussian has an associated high-dimensional **query vector** (feature) which now contains rich information. The model uses a small network (e.g. an MLP) to decode from this query feature a set of **intermediate properties** – basically a proposal for new Gaussian parameters (a new position, covariance, and semantic logits) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=aggregated%20sufficient%203D%20information%20in,layer%20perceptron%20%28MLP%29.%20When)). This is analogous to how an object detection transformer (DETR) would predict box adjustments from a query embedding ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Refinement%20Module,intermediate%20ones%2C%20we%20treat%20the)). In fact, the authors were *inspired by DETR* for this design ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Refinement%20Module,intermediate%20ones%2C%20we%20treat%20the)). However, instead of directly replacing the old properties with the new predictions, they found a more stable strategy: use a **residual update** for certain properties ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). Specifically, the **mean position** of the Gaussian is refined in a *relative* way – the network predicts an offset which is added to the old mean ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). This keeps the Gaussian's movement between iterations moderate and helps maintain coherence across blocks. For other properties like covariance and semantic logits, the update is done by direct substitution of the predicted values (with appropriate activation functions to keep them in valid ranges, e.g. a sigmoid to ensure covariance scales stay positive, and a softmax or similar for semantics) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=We%20refine%20the%20mean%20of,on%20covariance%20and%20semantic%20logits)). The authors note that if they tried to also update covariance in a residual manner, the gradients tended to vanish due to the sigmoid, and if they tried to update the mean by direct replacement, the Gaussians moved too erratically and the model collapsed ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=notable%20influence%20on%20the%20performance,the%20performance%20because%20it%20is)). The chosen hybrid approach – residual for position, direct for shape/label – proved crucial for stable training ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=properties%20with%20new%20ones%2C%20and,the%20overall%20performance%20by%20ensuring)). After this refinement step, we have a new set of Gaussian parameters that presumably better fit the scene than before the block. These updated Gaussians are then fed into the next block (going back to self-encoding with the new positions, etc.), and the process repeats for a total of $B$ blocks. Iteratively, the Gaussians "crawl" into alignment with objects and scene structures, and their semantic predictions become more accurate with each refinement, guided by intermediate supervision at each step (as we'll discuss in the training section) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=in%20the%20self,step%20benefits%20the%203D%20perception)).

**Gaussian-to-Voxel Splatting Module:** Once the final refined Gaussians are obtained after $B$ iterations, the last stage is to produce the dense 3D occupancy grid. The paper introduces an efficient **Gaussian-to-voxel splatting** algorithm to do this ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Gaussians%20and%20update%20them%20with,via%20local%20aggregation%20of%20Gaussians)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=the%203D%20semantic%20Gaussians%20can,using%20only%20local%20aggregation%20operation)). A straightforward implementation of Equation (3) – summing contributions of *all* Gaussians for each voxel – would be too slow (imagine thousands of Gaussians and hundreds of thousands of voxels) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Although%20,voxel%20position%20to%20improve%20efficiency)). Instead, the splatting module exploits the **locality** of Gaussians: each Gaussian only significantly affects a limited neighborhood of voxels ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Gaussians%20for%20every%20voxel%20position,voxel%20position%20to%20improve%20efficiency)). The procedure works in two main steps (illustrated in Figure 4): (1) **Neighbor list construction:** First, we determine for each Gaussian which voxels lie in its influence radius. They compute a radius for each Gaussian based on its scale (e.g. a few standard deviations out) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=As%20illustrated%20by%20Fig,each%20voxel%20should%20attend%20to)). Then, for each Gaussian $G_j$, they list all voxel indices $v$ within that radius and form pairs $(v, j)$ indicating "Gaussian $j$ contributes to voxel $v$". All such pairs for all Gaussians are collected and then sorted by the voxel index ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=the%20radius%20of%20its%20neighborhood,each%20voxel%20should%20attend%20to)). After sorting, we can easily gather the list of Gaussians that belong to each voxel's neighborhood. (2) **Local aggregation:** Then, for each voxel, the algorithm aggregates contributions from only its neighboring Gaussians (the list from step 1) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=the%20index%20of%20the%20Gaussian,each%20voxel%20should%20attend%20to)). It evaluates each Gaussian's formula $G_j(p)$ at the center of that voxel (the red star in Figure 4 right) and accumulates the semantic logits. In equation form, instead of $F(p)=\sum_i G_i(p)\,\text{softmax}(l_i)$ over all $i$, we restrict to $i \in \mathcal{N}(p)$, the Gaussians in the neighborhood of voxel $p$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). This yields an approximate occupancy value $F(p) \approx \sum_{i \in \mathcal{N}(p)} G_i(p)\,\text{softmax}(l_i)$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20%2C%20denote%20the%20number,efficiently%20with%20only%20neighboring%20Gaussians)), which is very accurate because Gaussians farther away contribute negligibly anyway. The clever indexing and sorting ensures we never iterate over a Gaussian-voxel pair that isn't local. The authors implemented this in **CUDA** (custom GPU code) to leverage parallelism – effectively, the GPU can handle many Gaussians' contributions in parallel, and sorting by voxel index can be done efficiently on GPU as well ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20represents%20the%20set%20of,language%20to%20achieve%20better%20acceleration)). This **parallel splatting** is fast and scalable even as the number of Gaussians grows, and it avoids the need for a huge dense tensor until the final result. The output of this module is a 3D tensor of size (X × Y × Z × C) – X,Y,Z voxels in each dimension, and C semantic classes – containing the predicted occupancy probabilities. In summary, Gaussian-to-voxel splatting bridges the gap between the sparse Gaussian world and the dense voxel world in an efficient manner, enabling the end-to-end training and evaluation of GaussianFormer on standard voxel-based benchmarks.

 ([GaussianFormer](https://wzzheng.net/GaussianFormer/)) *Figure 4: **Gaussian-to-Voxel Splatting (illustration in 2D for simplicity).** Each colored ellipse represents a 3D Gaussian (projected as an oval in this 2D slice). The dotted grid represents discrete voxels (in 2D these would be pixels). For a given Gaussian (e.g. *green* $G1$), we find all voxels within its influence radius (green shaded squares) and record those pairs $(G1, \text{voxel})$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=As%20illustrated%20by%20Fig,each%20voxel%20should%20attend%20to)). The list at top-left shows example pairs for Gaussian $G1$ (green), $G2$ (purple), $G3$ (blue) with various voxel indices. By sorting this list by voxel, we group pairs by voxel index (top-right). For example, voxel #13 (red star in right diagram) is inside both $G1$ and $G2$'s ellipses, so it has pairs $(G1,13)$ and $(G2,13)$ in the list ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=the%20radius%20of%20its%20neighborhood,each%20voxel%20should%20attend%20to)). The splatting module will aggregate contributions from $G1$ and $G2$ for voxel 13 – indicated by the arrows from those Gaussians to the red star ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=the%20index%20of%20the%20Gaussian,each%20voxel%20should%20attend%20to)). Each Gaussian's contribution is computed by plugging the voxel's coordinates into the Gaussian's formula (weight decays with distance), and added up to get the voxel's final semantic probabilities ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). This localized approach avoids summing over all Gaussians for every voxel, greatly improving efficiency.* ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Gaussians%20for%20every%20voxel%20position,voxel%20position%20to%20improve%20efficiency)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=))

<br>

# Computational Details

**Gaussian Initialization:** One hyperparameter of the model is the number of Gaussians $N$ to use for representing the scene. This is chosen based on a trade-off between accuracy and efficiency – more Gaussians can capture finer detail but cost more time/memory ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Effect%20of%20the%20number%20of,Gaussians%2C%20offering%20flexibility%20for%20deployment)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Number%20of%20Gaussians%20Latency%20Memory,83)). In the experiments, the authors used up to $N=144{,}000$ Gaussians for a large scene (nuScenes, 6 cameras) and around $N=38{,}400$ for a smaller monocular scene (KITTI-360) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=feature%20pyramid%20network%C2%A0,flip%20and%20photometric%20distortion%20augmentations)). These Gaussians are **initialized as learnable parameters** at the start of training ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=GaussianFormer,3)). That is, there isn't a fixed geometric placement – instead, the model has $N$ query embeddings each with an associated initial mean, scale, rotation, etc., and these are initially set (e.g. randomly or uniformly) and then learned via gradient descent. In practice, they mention "randomly initialized a set of queries and properties" ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=image%20inputs%20using%20an%20image,via%20local%20aggregation%20of%20Gaussians)). We can interpret this as the model starting with a random guess of where important regions might be, and gradually adjusting them to fit the training data (with the help of attention and loss feedback). Because of the large number of Gaussians, even a random initialization can cover the space roughly; during training the Gaussians will spread out and specialize to different parts of scenes to minimize the loss.

**Mathematical Representation of Gaussians:** Each Gaussian $G_i$ is represented by a vector of parameters $(\mu_i, s_i, q_i, l_i)$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=fixed%20grids%2C%20as%20shown%20in,distribution%20evaluated%20at%20point%20is)). Here $\mu_i \in \mathbb{R}^3$ is the mean (center coordinate). The covariance is derived from $s_i \in \mathbb{R}^3$ (scale) and $q_i$ (rotation). The paper uses $q_i$ likely as a quaternion to represent orientation; a quaternion is a 4D vector that can be converted to a 3×3 rotation matrix $R(q_i)$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20%2C%20and%20represent%20the,the%20occupancy%20prediction%20result%20at)). The scale $s_i$ might represent the Gaussian's standard deviation along its local x,y,z axes. So the covariance matrix is constructed as $\Sigma_i = R(q_i)\,\mathrm{Diag}(s_i^2)\,R(q_i)^T$, meaning you take the diagonal matrix of squared scales (making an ellipsoid aligned with the Gaussian's local axes) and rotate it into the world coordinate frame by $q_i$. Equation (1) and (2) of the paper formalize this (with $\mathrm{Diag}$ and quaternion-to-rotation noted) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). The semantic logits $l_i \in \mathbb{R}^C$ (where $C$ is number of classes) represent the class scores for that Gaussian. A softmax can convert $l_i$ to class probabilities. The **occupancy field** value at a point $p$ is then given by summing over Gaussians: $F(p) = \sum_{i=1}^N w_i(p)$, where $w_i(p) = G_i(p) \cdot \text{softmax}(l_i)$ is the contribution of Gaussian $i$ at point $p$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20%2C%20and%20represent%20the,Gaussians%20on%20the%20location)). $G_i(p)$ is the Gaussian weight (between 0 and 1) determined by distance from $\mu_i$ as per the Gaussian function. This summation (Equation (3) in the paper) essentially fuses all Gaussian "blobs" together at point $p$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20%2C%20and%20represent%20the,Gaussians%20on%20the%20location)). During inference, after splatting, each voxel will have a summed logit for each class from all nearby Gaussians ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). Taking an $\arg\max$ or softmax on those yields the final predicted class per voxel. 

One detail is that the Gaussian functions are not binary – they smoothly decay. So a single point in space could be partially occupied by multiple Gaussians (especially around object boundaries where Gaussians overlap). The training loss (which compares the final voxel predictions to ground truth) will encourage the Gaussians to adjust such that for each voxel, the correct class's sum dominates. This often leads to Gaussians specializing: e.g. one Gaussian will primarily cover a certain car, carrying the "car" label, etc., so that its contribution makes car-class logits high in that region. The *mixture of Gaussians* approach also inherently encodes uncertainty: if the model isn't sure, it could position a broader Gaussian or have multiple Gaussians of different class labels overlapping, which after softmax gives a mixed probability.

**Gaussian Property Refinement Equations:** As mentioned in the Refinement Module, the model updates Gaussian parameters iteratively. Let $(\mu_i^{(t)}, s_i^{(t)}, q_i^{(t)}, l_i^{(t)})$ be Gaussian $i$'s properties at iteration $t$. The refinement module produces some intermediate prediction $(\hat{\mu}_i, \hat{s}_i, \hat{q}_i, \hat{l}_i)$ from the query. The update rule they found best is: $\mu_i^{(t+1)} = \mu_i^{(t)} + \delta\mu_i$ (add the predicted offset) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)), and $s_i^{(t+1)} = \hat{s}_i$, $q_i^{(t+1)} = \hat{q}_i$, $l_i^{(t+1)} = \hat{l}_i$ (replace old covariance and semantics with the new predictions directly) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). In equation (5) they denote this substitution scheme ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=substitute%20the%20other%20intermediate%20properties,for%20the%20corresponding%20old%20properties)). By using a residual connection for position, the Gaussians don't "jump" too inconsistently from one iteration to the next ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=We%20refine%20the%20mean%20of,on%20covariance%20and%20semantic%20logits)). The direct replacement for the others is mainly to avoid vanishing gradients – since they apply sigmoid to $\hat{s}_i$ and softmax to $\hat{l}_i$ to keep them in valid ranges, trying to add a delta then clamp would complicate learning ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=We%20refine%20the%20mean%20of,on%20covariance%20and%20semantic%20logits)). This combination turned out to be important: an ablation study showed that if they *directly replaced all properties* (no residuals at all), the model collapsed and failed to learn (denoted as the "none" strategy) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Analysis%20of%20components%20of%20GaussianFormer,regularization%20for%20coherence%20during%20refinement)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=notable%20influence%20on%20the%20performance,the%20performance%20because%20it%20is)). And if they tried residuals on all except semantics (meaning residual update for both mean and covariance), it also hurt performance due to the covariance's sigmoid issue ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=properties%20with%20new%20ones%2C%20and,the%20overall%20performance%20by%20ensuring)). The chosen approach (residual on mean only) gave the best performance ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=%E2%9C%93%20%E2%9C%93%20none%20,37)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=trivial%20solution%20without%20regularization%20for,step%20benefits%20the%203D%20perception)), keeping Gaussians stable and optimizable.

**Sparse Convolution Efficiency:** Using **sparse conv for the self-encoding** stage is a key computational optimization. A naive self-attention among $N$ Gaussians would be $O(N^2)$ complexity. For $N=100k$, that's 10 billion interactions – not feasible. Sparse conv, by contrast, is roughly $O(N \cdot k)$ where $k$ is the number of neighbor voxels the conv kernel covers. If a small kernel (like $3^3$ neighborhood) is used, $k=27$ possible neighbor offsets – and since the space is sparse, only actual neighboring Gaussians count. Thus complexity is linear in $N$ (with a small constant) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Self,sparse%20convolution%20could%20effectively%20take)). The paper notes this shares the same linear complexity as deformable attention used in some prior works ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Self,sparse%20convolution%20could%20effectively%20take)). Additionally, modern sparse conv libraries are highly optimized in C++/CUDA to handle scatter/gather operations on sparse coordinates, so this operation can run very fast on GPU. In their ablations, they show that removing the sparse conv (i.e., not having a self-encoding stage) significantly degrades performance ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=except%20semantics,step%20benefits%20the%203D%20perception)) – indicating that letting Gaussians communicate is crucial for accuracy. Sparse conv provides that communication at a fraction of the cost of full attention.

**CUDA Implementation of Splatting:** The Gaussian-to-voxel splatting algorithm was implemented in CUDA to maximize parallelism ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=where%20represents%20the%20set%20of,language%20to%20achieve%20better%20acceleration)). Each voxel's computation can be done independently once the neighbor lists are prepared, which is ideal for GPU threads. The non-trivial part is building the neighbor lists efficiently. They voxelize Gaussian centers (to identify which voxel cell each Gaussian is in) and compute each Gaussian's radius in voxels ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=As%20illustrated%20by%20Fig,each%20voxel%20should%20attend%20to)). Then presumably a GPU kernel iterates through Gaussians: for each Gaussian, it marks all voxels in the cube (or sphere) of that radius around its mean and pairs them. Sorting these pairs by voxel can also be done by parallel sort (GPUs have efficient radix sort algorithms). After that, another kernel can accumulate each voxel's list. By doing this on GPU, they avoid transferring large lists to the CPU or any Python-level looping. The result is a very fast execution: Table 3 in the paper shows that GaussianFormer's splatting and overall pipeline achieves **372 ms** inference time with 144k Gaussians on an RTX 4090, which is faster than many prior occupancy methods that used dense grids ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Table%203%3A%20Efficiency%20comparison%20of,usage%20compared%20to%20other%20representations)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). Memory usage is also drastically lower thanks to not having to maintain full dense feature grids – at inference the main heavy memory use is storing the image feature maps and the Gaussian list, which was about 6.2 GB vs 25–35 GB for previous methods ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). Without the custom CUDA approach, such speed and memory efficiency would likely not be possible.

<br>

# Inference Pipeline

At inference (test) time, the GaussianFormer model takes a set of camera images as input and produces a 3D semantic occupancy grid as output. Here is a step-by-step breakdown of the process:

1. **Image Feature Extraction:** The multiple camera images (e.g. 6 surround-view images in nuScenes, or a single front image in KITTI-360) are first passed through a convolutional **image encoder** to extract features ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Image%3A%20Refer%20to%20caption%20Figure,via%20local%20aggregation%20of%20Gaussians)). In practice, GaussianFormer uses a ResNet backbone with a Feature Pyramid Network (FPN) to get multi-scale feature maps (with resolutions 1/4, 1/8, 1/16, 1/32 of the image) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=We%20set%20the%20resolutions%20of,with%20a%20weight%20decay%20of)). These multi-scale (M.S.) feature maps contain rich visual information about the scene from each view.

2. **Initialize 3D Gaussian Queries:** The model has a predetermined number $N$ of **3D Gaussian queries**, each with initial properties (position, size, orientation, semantic logits) and an associated query feature vector. These are initialized (learned) once and kept constant during inference. Essentially, at the start of inference, you can imagine we have $N$ Gaussians placed in the space (their initial positions might even be random or spread out) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=image%20inputs%20using%20an%20image,via%20local%20aggregation%20of%20Gaussians)). Each Gaussian $i$ also carries a query feature vector $\mathbf{q}_i$ (from training) which will be updated. Initially, these features might be just zero or random, but they are learned to be good starting points.

3. **Iterative Refinement Blocks:** The $N$ Gaussians are then refined through $B$ iterations (transformer decoder blocks). Each block consists of three sub-steps applied to all Gaussians:

   a. **Self-Encoding (Sparse Conv):** All Gaussians are projected into a sparse 3D grid based on their current mean positions ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=among%203D%20Gaussians%2C%20sharing%20the,of%20the%20sparsity%20of%20Gaussians)). A 3D sparse convolution is applied, allowing each Gaussian's feature to be updated with information from Gaussians in its vicinity (e.g. within a few voxels distance) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Self,sparse%20convolution%20could%20effectively%20take)). This lets neighboring Gaussians "talk" to each other – for instance, if a Gaussian is near another that has strong evidence of being a car, it can get some of that context. After the sparse conv, each Gaussian's query feature $\mathbf{q}_i$ is now contextually enriched.

   b. **Image Cross-Attention:** Next, each Gaussian query $\mathbf{q}_i$ attends to the image features. For Gaussian $i$, a set of 3D reference points are computed around its current mean $\mu_i$ (using offsets related to its covariance) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Image%20Cross,sum%20of%20retrieved%20image%20features)). These points are projected into each camera view using the known calibration (translation/rotation of camera and camera intrinsics) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=elaborate%2C%20for%20a%203D%20Gaussian,sum%20of%20retrieved%20image%20features)). The model then samples the multi-scale image feature maps at those projected 2D locations (often using bilinear interpolation). Those sampled features become the "values" for cross-attention. Essentially, each Gaussian pulls out the features from each image that correspond to its area of space. A deformable attention operation is used to aggregate these features: it computes attention weights and sums up the features to update the Gaussian's query vector ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=covariance%20of%20the%20Gaussian%20to,sum%20of%20retrieved%20image%20features)). After this step, $\mathbf{q}_i$ contains information from the images about what object or surface might be at $\mu_i$.

   c. **Refinement of Gaussian Properties:** Now the Gaussian's properties are adjusted. The query feature $\mathbf{q}_i$ (post-attention) is fed into an MLP that predicts an updated mean $\hat{\mu}_i$, scale $\hat{s}_i$, rotation $\hat{q}_i$, and class logits $\hat{l}_i$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Refinement%20Module,intermediate%20ones%2C%20we%20treat%20the)). The old mean is updated with a residual: $\mu_i \leftarrow \mu_i + \Delta \mu_i$ (where $\Delta \mu_i = \hat{\mu}_i - \mu_i$) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)), while the other properties are replaced: $s_i \leftarrow \hat{s}_i$, $q_i \leftarrow \hat{q}_i$, $l_i \leftarrow \hat{l}_i$ ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=)). This yields a new set of properties for Gaussian $i$ (now the $(t+1)$-th iteration). All Gaussians update in parallel like this. The refinement is guided by the cross-attention information; for example, if a Gaussian's image features clearly looked like part of a vehicle, its semantic logits $\hat{l}_i$ will shift more probability to the "vehicle" class, and its position might adjust to better center on that object.

4. **Repeat Refinement Blocks:** Steps 3a–3c are repeated for the configured number of blocks (e.g. $B=4$ times in the experiments ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=match%20at%20L431%20144000%20and,iterations%20to%20a%20maximum%20value))). With each iteration, the Gaussians' positions and features become more accurate. Early iterations make large adjustments (since initial guesses were random), later ones fine-tune (residual updates become smaller as the model converges on a solution). Because each block has its own parameters (like separate conv and attention layers), the process can be seen as a deep network unrolled over these iterations.

5. **Generate Occupancy Grid via Splatting:** After the final refinement, we have $N$ Gaussians with their final mean positions, covariances, and semantic class distributions. These are then converted to a 3D grid of size, say, $X \times Y \times Z$ (covering the driving area). The **Gaussian-to-voxel splatting** module takes each Gaussian and spreads its class probabilities to nearby voxels ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=As%20illustrated%20by%20Fig,each%20voxel%20should%20attend%20to)). As described earlier, for each voxel cell, the module finds all Gaussians within some neighborhood and accumulates their contributions ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=the%20index%20of%20the%20Gaussian,each%20voxel%20should%20attend%20to)). This produces for each voxel a set of class logits (sums of contributions from Gaussians of each class). The voxel's predicted class is the one with highest logit. The result is a dense 3D tensor of semantic predictions – effectively a **3D semantic occupancy map** of the scene. For example, a certain voxel at location (x,y,z) might end up with high score for "building" if several building-Gaussians cover it and no other Gaussians do. Another voxel on the road might get high "driveable surface" score from broad ground Gaussians.

6. **Post-processing (if any):** The paper doesn't highlight significant post-processing; the output grid can be directly evaluated against ground truth. One could optionally apply a threshold if focusing just on occupancy vs free space, or combine adjacent voxels of the same class for visualization, but these are not part of the learned model.

The above pipeline turns raw images into an understanding of the 3D world. Thanks to the Gaussian representation, the intermediate steps operate in a sparse domain (only N Gaussians rather than millions of voxels), and thanks to attention, the model intelligently associates image evidence to 3D locations. By the end, the splatted voxel grid can be used for tasks like path planning, collision checking, or simply visualized for the human to verify what the car "sees" in 3D.

<br>

# Training Pipeline

**Datasets and Input Setup:** GaussianFormer is trained and evaluated on two main datasets: **nuScenes** and **KITTI-360** ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=nuScenes%C2%A0%5B3%5D%20dataset%20and%20the%20KITTI,surrounding%20and%20monocular%20cameras%2C%20respectively)). NuScenes is a large-scale autonomous driving dataset with 6 surround RGB cameras (covering 360°) capturing urban scenes ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=NuScenes%C2%A0,surrounding%20cameras%2C%20and%20the%20keyframes)). Ground-truth 3D occupancy for nuScenes is derived from LiDAR scans; recent works (like SurroundOcc) provide dense occupancy annotations by aggregating LiDAR points over time into a voxel grid ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=OccFormer%C2%A0,12)). KITTI-360 is a smaller dataset with a monocular camera moving through city streets, also with 3D annotations of the environment. For training, the multi-camera images are taken at keyframes and the corresponding ground truth occupancy grid is used as supervision.

All input images are resized to a fixed resolution for training (to fit GPU memory). The paper mentions using **$1600\times900$ resolution for nuScenes and $1408\times376$ for KITTI-360** ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=We%20set%20the%20resolutions%20of,with%20a%20weight%20decay%20of)) (these numbers correspond to commonly used image sizes in those benchmarks). Each image is passed through the backbone (ResNet101-DCN for nuScenes, ResNet50 for KITTI-360) which may be initialized from a pretrained model (they used a FCOS3D pretrained model for nuScenes to give the backbone a head-start in 3D understanding) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=We%20set%20the%20resolutions%20of,For)). The FPN then produces multi-scale features. These features, along with camera calibration info (intrinsics/extrinsics), are provided to the GaussianFormer modules.

**Loss Functions:** Training is supervised by comparing the predicted occupancy grid to the ground truth occupancy at every voxel. This is essentially a **3D semantic segmentation** task, so GaussianFormer uses a **cross-entropy loss** for semantic classification of each voxel ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=The%20overall%20GaussianFormer%20model%20can,th%20block)). Cross-entropy is the standard loss for multi-class prediction, encouraging the correct class probability to be high. However, 3D occupancy data is highly imbalanced – most voxels in the volume are empty or irrelevant (e.g. air or background) and only a fraction contain important objects or structures. To better handle this, the model also uses the **Lovász-Softmax loss** ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=The%20overall%20GaussianFormer%20model%20can,th%20block)). Lovász-Softmax is a surrogate for the Intersection-over-Union (IoU) metric; it is often used in segmentation tasks to directly optimize IoU, which is more sensitive to class imbalance ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=The%20overall%20GaussianFormer%20model%20can,th%20block)). By using this loss in addition to cross-entropy, the training rewards the model for improving IoU of each class – for example, even if a class like pedestrian occupies very few voxels, the Lovász loss helps ensure those voxels are accurately predicted, rather than being overwhelmed by the majority class (empty or road). The combination of cross-entropy and Lovász-softmax was used in prior occupancy methods like TPVFormer as well ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=manner,th%20block)), indicating it's an effective strategy.

**Iterative Deep Supervision:** A noteworthy aspect of training is that supervision is applied not just at the final output, but also at intermediate refinement steps. The model produces a full occupancy prediction after each refinement block (by splatting the Gaussians from that block) and compares each to the ground truth, with losses computed at each stage ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=The%20overall%20GaussianFormer%20model%20can,th%20block)). If the model has $B$ blocks, and we denote the output after block $k$ as $O_k$, then the total loss is $L = \sum_{k=1}^{B} \lambda_k \, \mathcal{L}(O_k, Y)$, where $Y$ is ground truth and $\mathcal{L}$ is the combined CE + Lovász loss. In the paper they indicate the overall loss is the sum of losses at each block (they did not mention using different weights $\lambda_k$, so likely each is equally weighted) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=The%20overall%20GaussianFormer%20model%20can,th%20block)). This **deep supervision** helps convergence by guiding the model at multiple scales of refinement ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=in%20the%20self,step%20benefits%20the%203D%20perception)). Early blocks get feedback to start moving Gaussians in the right direction rather than waiting until the very end to see if the final output is correct. As evidence, the ablation study showed that removing deep supervision (only supervising final output) hurt performance ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Deep%20Supervision%20Sparse%20Conv,37)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=in%20the%20self,step%20benefits%20the%203D%20perception)) – every intermediate refinement contributes to learning a better representation.

**Optimization and Hyperparameters:** The model is trained end-to-end using **AdamW optimizer** (Adam with weight decay) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=144000%20and%2038400%20for%20nuScenes,flip%20and%20photometric%20distortion%20augmentations)). The weight decay is 0.01 to regularize the model weights. They use a learning rate schedule with a warm-up phase: the LR linearly increases to $2\times10^{-4}$ over the first 500 iterations, then follows a cosine decay schedule ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=144000%20and%2038400%20for%20nuScenes,flip%20and%20photometric%20distortion%20augmentations)). This is a common schedule to stabilize early training and then gradually reduce the learning rate for fine tuning. The models were trained for **20 epochs** on nuScenes (which with 700 training scenes and certain frame sampling likely corresponds to around 144k iterations as mentioned) and similarly on KITTI-360 with fewer iterations (since KITTI-360 is smaller) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=match%20at%20L431%20144000%20and,iterations%20to%20a%20maximum%20value)). They used a batch size of 8 (probably meaning 8 scenes or 8 training examples per batch) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=optimization%2C%20we%20utilize%20the%20AdamW%C2%A0,flip%20and%20photometric%20distortion%20augmentations)). As for data augmentation, they applied random horizontal flips of images and photometric distortions (color jitter, etc.) during training ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=optimization%2C%20we%20utilize%20the%20AdamW%C2%A0,flip%20and%20photometric%20distortion%20augmentations)). These augmentations help the model generalize to different lighting and orientations.

**Convergence and Stability Tricks:** Training such a complex model with many learnable Gaussians could be unstable. The authors highlighted a few strategies that made it work:

- **Residual Mean Update:** Using the residual refinement for the mean (position) of Gaussians, as discussed, was crucial. Without it, they observed the Gaussians quickly converged to a bad, trivial solution (e.g. clumping together or all predicting "empty") ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=notable%20influence%20on%20the%20performance,the%20performance%20because%20it%20is)). The residual update acted as a regularizer, keeping Gaussians from drifting too far and enforcing a sort of temporal consistency across iterations.

- **Proper Handling of Covariance Activation:** They found that updating covariance with a direct residual caused vanishing gradients due to the sigmoid. By outputting absolute scale values each time, they ensured gradients flow. This detail, while small, affected training – it prevented the network from getting stuck when adjusting Gaussian sizes ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=properties%20with%20new%20ones%2C%20and,the%20overall%20performance%20by%20ensuring)).

- **Sufficient Number of Gaussians:** If $N$ (Gaussians count) is too low, the model might not have enough "capacity" to represent the scene and will have high error that it can't reduce, leading to poor local minima. The authors did an ablation where they vary $N$ and found performance improves as $N$ increases, especially beyond a certain point ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Effect%20of%20the%20number%20of,Gaussians%2C%20offering%20flexibility%20for%20deployment)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Number%20of%20Gaussians%20Latency%20Memory,83)). They set $N$ generously (144k for nuScenes) to ensure the model can fit the data well. This many Gaussians can still be trained because each carries gradients for its own parameters and they share the attention and conv layers.

- **Gradient scaling and normalization:** Though not explicitly mentioned, in such transformer-based setups it's typical to use layer normalization in the attention layers and perhaps gradient clipping to avoid exploding gradients. We can infer they likely used standard transformer practices which aid training convergence.

Despite these measures, the authors note they did not heavily tune hyperparameters ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Limitations,further%20improve%20performance%20and%20speed)). There might be further room to improve training stability and final accuracy with more careful tuning or advanced techniques, but even with their setup, the model was able to learn effectively. They trained on powerful GPUs (a 4090 or A100 as mentioned for benchmarking) for efficiency.

In summary, the training pipeline uses **multi-task losses (occupancy classification + IoU optimization)** and **iterative supervision** to gradually teach the Gaussians to shape themselves into the ground truth scene. Over epochs, each Gaussian finds a "role" (e.g. representing a part of a sidewalk or a vehicle) such that collectively they minimize the loss. By the end of training, the model has learned to place Gaussians in meaningful locations and assign them correct semantic labels purely from image cues.

<br>

# Comparisons and Results

**Accuracy Performance:** GaussianFormer was evaluated on the nuScenes 3D occupancy benchmark and the KITTI-360 semantic scene completion benchmark. On **nuScenes**, it achieves accuracy on par with the state-of-the-art. For instance, GaussianFormer's *semantic scene completion mIoU* (mean Intersection-over-Union over all classes) on nuScenes validation is about **19.1%**, compared to 20.3% for the best prior method (SurroundOcc) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=OccFormer%C2%A0,12)). Its *occupancy IoU* (binary free vs occupied IoU, sometimes called scene completion IoU) is **29.8%**, which is also close to top methods (SurroundOcc ~31.5%) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=OccFormer%C2%A0,12)). In practical terms, this means it correctly labels a very similar proportion of voxels as those top models. Notably, GaussianFormer **outperforms methods based on planar (2D) representations** like BEVFormer and TPVFormer by a significant margin ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=semantic%20occupancy%20prediction%20on%20nuScenes,compared%20with%20dense%20grid%20representations)). Those methods had mIoUs in the mid-teens (BEVFormer ~16.8, TPVFormer* ~17.1), whereas GaussianFormer is ~19.1 ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Atlas%C2%A0,35)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Ours%2029,12)). This shows the strength of the 3D Gaussian approach in preserving detail that planar methods lose. Compared to dense 3D grid methods (OccFormer, SurroundOcc), GaussianFormer is very competitive – within ~1-1.5 points mIoU, even slightly surpassing some on certain classes ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=OccFormer%C2%A0,12)). On **KITTI-360**, GaussianFormer similarly matches state-of-the-art. Table 2 of the paper shows it achieving comparable mIoU to prior methods on that dataset ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Ours%2029,12)). The authors highlight that their method especially excels on smaller object classes like *motorcycle* and *other-vehicle* categories in KITTI-360 ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Table%202%3A%203D%20semantic%20occupancy,veh)), likely because the adaptive Gaussians can allocate more resolution to small objects.

**Qualitative Results:** The paper's visualizations (Fig. 5, 6, 7) show that GaussianFormer produces very plausible 3D reconstructions ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Visualization%20results,visible%20in%20the%20images%2C%20such)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Figure%206%3A%20%20Visualizations%20of,right)). The predicted 3D occupancy maps look consistent with the scene – buildings and walls are reconstructed as solid surfaces, cars as distinct clusters of occupied voxels, etc. They even note cases where the model predicts objects that were not labeled in ground truth (e.g. it hallucinated a truck in an unlabeled area because it saw something in the images) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=road%20surface%20%28e,visualizations%20in%20the%20fourth%20row)). This suggests the model is sometimes picking up on visual cues beyond the provided annotation, which could be a positive sign of generalization (though it hurts measured accuracy if predicting things not in ground truth). The **3D Gaussians** themselves can be visualized by drawing each as an ellipsoid. The authors show that Gaussians tend to cluster and shape themselves appropriately: e.g. along a flat road, they become flattened disk-like ellipsoids (capturing the planar road) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=scene,right%20corner%20of%20the%203D)), and around objects like cars, many Gaussians cluster to outline the shape (with higher density of Gaussians where detail is needed, such as a pedestrian's shape getting multiple small Gaussians) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=scene,right%20corner%20of%20the%203D)). This behavior demonstrates the intended effect: more Gaussians in complex regions, fewer in simple regions, achieving efficient representation.

**Memory and Latency Efficiency:** One of the biggest advantages of GaussianFormer is its **dramatic reduction in memory usage**. Traditional dense voxel methods need to store large 3D feature grids or multiple plane features, which can consume 25–35 GB of memory for a single inference, as shown in Table 3 ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). GaussianFormer, in contrast, only stores on the order of $N$ Gaussian descriptors and uses sparse data structures. In the implementation, it used about **6229 MB (≈6.2 GB)** of GPU memory for a scene (with 144k Gaussians) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). That's roughly *one-fifth* or *one-quarter* of the memory of previous methods (17.8%–24.8% as stated) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2405.17429#:~:text=neighboring%20Gaussians%20for%20a%20certain,available%20at%3A%20this%20https%20URL)). This is a **75–82% reduction in memory** ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=use%20of%20dense%20grids,82.2)), which is significant for deploying such models on real vehicles or on limited hardware. In terms of **inference speed (latency)**, GaussianFormer is also quite competitive. With the full pipeline implemented (including the custom CUDA splatting), it runs in **~372 ms per frame** on an NVIDIA 4090 GPU ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Table%203%3A%20Efficiency%20comparison%20of,usage%20compared%20to%20other%20representations)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). This is actually faster than most dense 3D methods – e.g. a voxel method like PanoOcc took 502 ms, FBOcc 463 ms, OccFormer ~386 ms (on A100) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). GaussianFormer even beats Octree-based acceleration (OctreeOcc was 386 ms) while using more queries ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=representation%20to%20occupancy%20predictions%2C%20getting,33%5D%20even%20with%20more%20queries)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). It is only slightly slower than BEVFormer (302 ms) and TPVFormer (341 ms) which use simpler 2D processing ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). Considering those planar methods are less accurate, a small trade-off in speed for much better detail is warranted. Moreover, the authors tested on a 4090 (which is a bit less powerful than an A100 used for others), so GaussianFormer's speed is actually very good. These results underscore that the object-centric sparse processing is not just memory-efficient but also allows parallel GPU execution effectively.

To quantify: **BEVFormer** needed 25 GB and 302 ms ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)), **TPVFormer** 29 GB and 341 ms, **OccFormer** ~30 GB and ~386 ms, whereas **GaussianFormer** uses ~6 GB and 372 ms for a similar output ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Methods%20Query%20Form%20Query%20Resolution,372%20ms%20%206229%20M)). This makes GaussianFormer much more practical for deployment, as many GPUs (especially in cars or edge devices) might not even have 30 GB available.

**Ablation Studies:** The paper includes ablations to justify design choices. In Table 4, they tested the effect of *Deep Supervision*, *Sparse Conv (self-encoding)*, and *Residual Refinement strategy* ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Table%204%3A%20%20Ablation%20on,refinement%20as%20opposed%20to%20substitution)). They found that **removing sparse conv** (i.e. no self-encoding, using "deformable attention" or nothing among Gaussians) caused a complete failure ("collapse") ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=%E2%9C%93%20%E2%9C%93%20none%20,37)) – indicating that letting Gaussians communicate via sparse conv is *crucial* for good performance ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=except%20semantics,step%20benefits%20the%203D%20perception)). **Removing deep supervision** (only supervising final output) led to a drop in mIoU (from 16.41 to 15.93 in a certain setting) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Deep%20Supervision%20Sparse%20Conv,37)), showing that intermediate losses do help each stage learn better and improve overall accuracy ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=in%20the%20self,step%20benefits%20the%203D%20perception)). For refinement strategy, as discussed, the "none" (no residuals) collapsed ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=notable%20influence%20on%20the%20performance,the%20performance%20because%20it%20is)), and "all except semantics residual" (i.e. residual for mean and covariance) also hurt a bit, whereas the chosen "mean only residual" gave the best result ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=%E2%9C%93%20%E2%9C%93%20none%20,37)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=trivial%20solution%20without%20regularization%20for,step%20benefits%20the%203D%20perception)). These ablations validate the architectural decisions.

They also ablated the **number of Gaussians** (Table 5) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Table%205%3A%20%20Ablation%20on,up%20more%20time%20and%20memory)). With too few Gaussians (e.g. 25k), the mIoU was much lower ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Number%20of%20Gaussians%20Latency%20Memory,83)). Increasing to 51k gave a decent jump, and up to 144k continued to improve mIoU (reaching 19.10% with 144k on nuScenes val) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Number%20of%20Gaussians%20Latency%20Memory,83)). The IoU (binary occupancy) also improved, but interestingly there was a dip at 91k in their table for binary IoU ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=38400%20249%20ms%204856%20M,83)), which might be noise or specific difficulty. The trend is clear though: **more Gaussians = better performance**, roughly linearly until a saturation point ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Effect%20of%20the%20number%20of,Gaussians%2C%20offering%20flexibility%20for%20deployment)). Along with this, they reported the latency and memory for each – which grew linearly with number of Gaussians (e.g. 227 ms at 25k up to 372 ms at 144k; memory 4.85 GB to 6.23 GB) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Number%20of%20Gaussians%20Latency%20Memory,83)). This linear scaling offers a nice *knob* for deployment: one could reduce $N$ to speed up at cost of some accuracy, or increase if resources allow to maximize accuracy ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Effect%20of%20the%20number%20of,Gaussians%2C%20offering%20flexibility%20for%20deployment)) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=performance,Gaussians%2C%20offering%20flexibility%20for%20deployment)). The model doesn't fundamentally break with fewer Gaussians; it just has lower resolution in output.

**Comparison to Related Methods:** In the paper, they compare conceptually to grid vs planar which we already covered. It's worth noting that GaussianFormer is the first to use Gaussians as the representation for this task (previous methods used voxel grids, or planar decompositions, or an octree structure). This approach shares spirit with **object detection** (predicting objects) but instead of discrete bounding boxes, it predicts a field of Gaussians that cover the entire space. A contemporaneous method "OctreeOcc" built an octree to subdivide space adaptively ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=method%20is%20faster%20than%20OctreeOcc%C2%A0,even%20with%20more%20queries)); GaussianFormer achieves similar adaptivity but uses a continuous parametric shape (Gaussian) rather than an axis-aligned cube subdivision.

Overall, **GaussianFormer's results** demonstrate that an object-centric, sparse representation can **match the accuracy of dense methods while massively improving efficiency**. This is a promising validation of the idea that we can rethink the representation for scene understanding – we don't need to brute-force fill the space with voxels if we can intelligently place a few adaptable elements (Gaussians). The occupancy predictions from GaussianFormer are nearly as good, and in some cases better (especially on small objects and fine details) ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Table%202%3A%203D%20semantic%20occupancy,veh)), compared to strong baselines, making it a very attractive approach for vision-based 3D perception.

<br>

# Limitations and Future Work

While GaussianFormer shows strong efficiency and solid performance, the authors acknowledge some **limitations** and areas for improvement ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Limitations,further%20improve%20performance%20and%20speed)):

- **Remaining Performance Gap:** There is still a small gap between GaussianFormer and the absolute top-performing dense methods (e.g., on nuScenes SurroundOcc had ~1.2 higher mIoU). GaussianFormer's **accuracy is slightly lower** than the best, which could be due to how it represents the scene. If a region is not well-covered by the Gaussians (e.g. an oddly shaped object or a very thin structure), the prediction might be less precise than a dense voxel model that had a cell for every spot. The authors suggest this *"might result from the inaccuracy of the 3D semantic Gaussian representation or simply the wrong choice of hyperparameter"* ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Limitations,further%20improve%20performance%20and%20speed)). In other words, maybe the Gaussians as formulated cannot perfectly model some complex geometries, or perhaps with more tuning (learning rate, more blocks, etc.) they could close the gap. Future work could explore improved Gaussian parameterizations or initialization schemes to boost accuracy.

- **High Number of Gaussians Required:** To reach high accuracy, GaussianFormer still needed a fairly large number of Gaussians (up to 144k in nuScenes). This is much sparser than millions of voxels, but 144k is not a trivial number of entities. The authors note that the current representation includes "**empty** as one category" for Gaussians ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=from%20the%20inaccuracy%20of%203D,further%20improve%20performance%20and%20speed)). That means many Gaussians might end up representing empty space (background) rather than actual objects. Those Gaussians are somewhat wasted from an efficiency standpoint – they're modeling nothingness. This suggests **redundancy** still exists: ideally we'd only have Gaussians where something is there. They mention it would be interesting to *"only model solid objects"* in the future ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=GaussianFormer%20also%20requires%20a%20large,further%20improve%20performance%20and%20speed)), i.e. have Gaussians focus on occupied regions and maybe handle free space differently. If one could prune away Gaussians that try to model empty regions, or dynamically allocate Gaussians only where needed, that could reduce $N$ further and improve speed.

- **Dynamic Scene Changes / Motion:** The paper doesn't explicitly discuss this, but one limitation of the current setup is that it operates per frame (for nuScenes, each keyframe). In a dynamic environment, moving objects are captured in the occupancy at an instant, but the model doesn't yet incorporate temporal tracking of Gaussians. Future work might extend GaussianFormer to track Gaussians over time (making them persist and move) to stabilize predictions in video.

- **LiDAR or Sensor Fusion:** GaussianFormer is vision-centric. One could consider fusing LiDAR data or using LiDAR to initialize Gaussians (since LiDAR gives precise 3D points). The current model doesn't do this, but future research might integrate Gaussians with point clouds – e.g. using Gaussians to represent clusters of lidar points (there is related work in other contexts on fitting Gaussians to point clouds). This could improve accuracy dramatically by leveraging depth sensors.

- **Gaussian Split & Merge:** In the appendix, they allude to experiments on "splitting & pruning strategy" ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=In%20Fig,144000%20%2F%2038400%20and%20the)). This points to a potential improvement: allowing Gaussians to **split** into multiple if one Gaussian is covering a region that would be better represented by two (increasing resolution where needed), or **prune** Gaussians that are not contributing (e.g. a Gaussian consistently predicts empty, it could be removed). A smart algorithm to adapt the number of Gaussians per scene (instead of a fixed $N$) could make the representation more efficient and perhaps more accurate. The paper did not implement a dynamic split/merge during training (all Gaussians are fixed count), so this is a ripe area for future work.

- **Hyperparameter Tuning:** The authors admitted they did not do extensive hyperparameter tuning ([[2405.17429] GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/pdf/2405.17429#:~:text=Limitations,further%20improve%20performance%20and%20speed)). Future efforts could likely get better results by trying different learning rates, number of refinement blocks, network sizes, or using advanced training tricks (like curriculum learning: start with a coarse grid then gradually increase Gaussian count, etc.). Also, the choice of 4 blocks was somewhat arbitrary – more blocks might allow more refinement at the cost of speed.

- **Modeling "Empty" Differently:** As mentioned, one future idea is to not model empty space with Gaussians. Perhaps one could have a default assumption of empty everywhere and only model occupied regions with Gaussians (like placing Gaussians for objects and surfaces). This might require a different loss formulation, since currently every voxel's class is explained by Gaussians. If empty space had no Gaussians, how do we ensure those voxels get labeled empty? Possibly a background Gaussian or a fallback value could be used. Or one could start with a base dense field for empty and add Gaussians for occupied additions. Research could explore hybrid representations.

- **Expressiveness of Gaussians:** Gaussians are smooth and uni-modal; a single Gaussian can't represent a concave shape or multiple disjoint bits. While mixtures handle this by using many Gaussians, there might be cases where even with many Gaussians, representing very sharp edges or high-frequency detail is challenging. Future work might consider other primitives (e.g. learned implicit functions or anisotropic Gaussians with more complex lobes) to increase expressiveness without too many components.

In conclusion, *GaussianFormer* is a novel step towards more efficient 3D perception. It significantly reduces memory usage and maintains good accuracy by rethinking the representation. Its limitations open up interesting research directions: making the Gaussian representation even more adaptive (with dynamic counts or focusing only on occupied space) and pushing its performance to exceed dense grids. The concept of treating a scene as a collection of geometric primitives (Gaussians) learned from images could be extended beyond occupancy to tasks like multi-object tracking (each Gaussian following an object) or even mapping (accumulating Gaussians over time to build a world model). The authors have provided code, so we may see follow-up works building on GaussianFormer to address these limitations and refine the approach further.

